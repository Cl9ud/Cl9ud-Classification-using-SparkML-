#Installing Required Libraries
!pip install pyspark==3.1.2 -q
!pip install findspark -q

#Importing Required Libraries
# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# FindSpark simplifies the process of using Apache Spark with Python

import findspark
findspark.init()

from pyspark.sql import SparkSession

#import functions/Classes for sparkml

from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression

# import functions/Classes for metrics
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

#Create a spark session
#Create SparkSession
#Ignore any warnings by SparkSession command

spark = SparkSession.builder.appName("Classification using SparkML").getOrCreate()

#Load the data in a csv file into a dataframe
!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/drybeans.csv

#Load the dataset into the spark dataframe
# using the spark.read.csv function we load the data into a dataframe.
# the header = True mentions that there is a header row in out csv file
# the inferSchema = True, tells spark to automatically find out the data types of the columns.

# Load mpg dataset
beans_data = spark.read.csv("drybeans.csv", header=True, inferSchema=True)

#Print the schema of the dataset
beans_data.printSchema()

#Print top 5 rows of selected columns from the dataset
beans_data.select(["Area","Perimeter","Solidity","roundness","Compactness","Class"]).show(5)

#Print the value counts for the column 'Class'
beans_data.groupBy('Class').count().orderBy('count').show()

#Convert the string column "Class" into a numberic column named "Label"
# Convert Class column from string to numerical values
indexer = StringIndexer(inputCol="Class", outputCol="label")
beans_data = indexer.fit(beans_data).transform(beans_data)

#Print the value counts for the column 'label'
beans_data.groupBy('label').count().orderBy('count').show()

#Identify the label column and the input columns
#ask the VectorAssembler to group a bunch of inputCols as single column named "features"
#Use "Area","Perimeter","Solidity","roundness","Compactness" as input columns

# Prepare feature vector
assembler = VectorAssembler(inputCols=["Area","Perimeter","Solidity","roundness","Compactness"], outputCol="features")
beans_transformed_data = assembler.transform(beans_data)

#Display the assembled "features" and the label column "MPG"
beans_transformed_data.select("features","label").show()

#Split the data
#split the data set in the ratio of 70:30. 70% training data, 30% testing data.
# Split data into training and testing sets
(training_data, testing_data) = beans_transformed_data.randomSplit([0.7, 0.3], seed=42)

#Build and Train a Logistic Regression Model
#Create a LR model and train the model using the training data set
# Ignore any warnings

lr = LogisticRegression(featuresCol="features", labelCol="label")
model = lr.fit(training_data)

#Evaluate the model
#the model is now trained. We use the testing data to make predictions.
# Make predictions on testing data
predictions = model.transform(testing_data)

#Accuracy
# Evaluate model performance
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print("Accuracy =", accuracy)

#Precision
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedPrecision")
precision = evaluator.evaluate(predictions)
print("Precision =", precision)

#Recall
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedRecall")
recall = evaluator.evaluate(predictions)
print("Recall =", recall)

#F1 Score
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")
f1_score = evaluator.evaluate(predictions)
print("F1 score = ", f1_score)

#Stop Spark Session
spark.stop()
